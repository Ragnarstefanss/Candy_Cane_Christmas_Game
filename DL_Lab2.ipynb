{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhCxg065q3bx9skzK3oTPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ragnarstefanss/Candy_Cane_Christmas_Game/blob/master/DL_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWnB13vva24w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz5JoyHGcF5l"
      },
      "source": [
        "The task of this lab is to read external data into a dataset that you use to train and test a simple ANN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2VIJUcVcu8U",
        "outputId": "1e51ebb4-62a7-4071-8a34-9c41d86aff11"
      },
      "source": [
        "from google.colab import drive\n",
        "from os.path import join\n",
        "\n",
        "ROOT='/content/drive'\n",
        "drive.mount(ROOT)\n",
        "PROJ='MyDrive/DL2020/lab2'\n",
        "PROJ_PATH = join(ROOT,PROJ)\n",
        "\n",
        "!rsync -aP \"{PROJ_PATH}\"/* ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "sending incremental file list\n",
            "iris_test_dataset.txt\n",
            "            660 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=1/2)\n",
            "iris_train_dataset.txt\n",
            "          2,641 100%    5.29kB/s    0:00:00 (xfr#2, to-chk=0/2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwh7NGGDf5OD"
      },
      "source": [
        "# Read test and training data from files into a dataset.\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# Read data into a numpy array.\n",
        "train_data = np.loadtxt('iris_train_dataset.txt', delimiter=',')\n",
        "test_data = np.loadtxt('iris_test_dataset.txt', delimiter=',')\n",
        "#print(test_data)\n",
        "\n",
        "t_train_X = torch.Tensor(train_data[:, 0:4])\n",
        "t_train_y = torch.argmax(torch.Tensor(train_data[:, 4:7]), dim=1)\n",
        "t_test_X = torch.Tensor(test_data[:, 0:4])\n",
        "t_test_y = torch.argmax(torch.Tensor(test_data[:, 4:7]), dim=1)\n",
        "#print(t_test_X)\n",
        "#print(t_test_y)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(t_train_X, t_train_y)\n",
        "test_dataset = torch.utils.data.TensorDataset(t_test_X, t_test_y)\n",
        "\n",
        "train_set = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_set = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}